\documentclass[a4paper, 11pt]{article}
%\usepackage[margin=0.8in]{geometry}
%\usepackage[T1]{fontenc}
%\usepackage{babel}
%\setcounter{section}{6}
\usepackage{times,amsmath,epsfig}
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{amssymb}

\let\labelindent\relax

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{pdfpages}
\usepackage{fancyhdr}
\usepackage{exscale}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{ntheorem}
\usepackage{paralist}
\usepackage{verbatim}
\usepackage{color}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{listings}      % source code
\usepackage{algorithm}
\usepackage{algorithmicx}  % pseudo code (1/2)
\usepackage[noend]{algpseudocode} % pseudo code (2/2)
\usepackage{setspace}
\usepackage{tikz}
\usepackage{array}
\usepackage{relsize}
\usepackage{diagbox}
\usepackage{listings}


\DeclareMathOperator{\union}{\mathtt{union}}
\providecommand{\myfloor}[1]{\left \lfloor #1 \right \rfloor}

%%%%%%TRIGGER
\algblockdefx[TriggerS]{TriggerS}{EndTriggerS}[2][event]
  {\textbf{trigger} $\langle #1 \rangle$;}

%No extra line - TriggerS 
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndTriggerS}}
  {}%
\makeatother

\algblockdefx[Trigger]{Trigger}{EndTrigger}[2][event]
  {\textbf{trigger} $\langle#1$ $\mid$ $#2\rangle$;}

%No extra line - Trigger  
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndTrigger}}
  {}%
\makeatother

%%%%%%FOR EACH
\algblockdefx[ForEach]{ForEach}{EndForEach}[2][collection]
   {\textbf{ for each } $#1$ \textbf{as} $#2$}

%No extra line - ForEach
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndForEach}}
  {}%
\makeatother

%%%%%%UPON
\algblockdefx[UponEvent]{Upon}{EndUpon}[2][event]
  {\textbf{upon event} $\langle#1$ $\mid$ \emph{#2}$\rangle$ \textbf{do} }

\algblockdefx[UponEventS]{UponS}{EndUponS}[2][event]
  {\textbf{upon event} $\langle#1\rangle$ \textbf{do} }



\author{
  Babbar, Abhimanyu\\
  \textsc{890729-7751}\\
  \texttt{babbar@kth.se}
}
\title{Network Partition Scenarios} 

\begin{document}

\maketitle 

\section{Scenarios}
Analysis of the different Network Partition Scenarios and there potential solutions.

\subsection{Very Short Lived Partition}
The basic and a very frequent scenario that would occur would be that for a very short duration of time, a set of nodes gets networked partitioned. The duration is small enough so that the nodes that got partitioned \textbf{are unable to elect a new leader}.\\

\textbf{Solution}: The nodes simply merge back without any hiccups and start working as before and fetch the entries if any that could have been added in that short duration.


\subsection{Short Lived Partition}
A scenario in which the nodes partition in separate branches and diverge from a common point. The stale descriptors for some time will be present in the gradient and might be buffered in the communication maps of the application. In this, it might be possible that the other partition moves ahead and its leader unit history evolves but the partition heals before the application in the other branch is able to clean the partitioned nodes. \\

\textbf{Solution}: The solution for this case will be tricky because the nodes will still contain stale samples from other nodes in the system and in case the partition heals without the samples being removed from the node the node will contact with the nodes. Therefore more tight control needs to be imposed on every message exchange in terms of introducing last leader unit in every exchange to prevent them to serve the data which does not belong to the current network partition. It will pollute the partition.

\subsection{Medium Lived Partition without External Intervention}
Scenario in which the nodes partition in separate branches but the time for which the nodes partition is large enough so that the samples became stale enough to be removed from the gradient. As gradient implements an age threshold mechanism which simply removes all the samples from the gradient which are above a certain age. In addition to this, we assume that the stale samples are present in croupier and therefore when the partition merges back in the croupier component communicates with the samples and when response is received, the samples start getting pushed to the gradient with low ages. The gradient then filters these samples and inform application about the partitioned nodes.

\subsection{Medium Lived Partitioned With 3rd Party Intervention}
In this scenario, \textit{caracal} is used as an arbitrator and nodes constantly ping the caracal for checking for the network partition. The system when getting network partitioned, one branch of the partitioned is unable to ping the caracal and detect it as a network partition. When assume that the stale descriptors are removed from application, gradient and croupier. Therefore on partition merge, the croupier needs to be bootstrapped with the nodes from other partition and this is place where caracal is useful as it provides the nodes from the other partition on heartbeat response or separately when asked explicitly.



\subsection{General Network Partition Without Sharding}
The main case that needs to be handled as part of thesis scope is that nodes gets networked partitioned for a medium to long duration of time but the partition heals before any set of nodes reaches the sharding stage. The set of nodes act independently and the nodes which got separated from the leader, thinks that the leader is dead and therefore elects a new leader. Both the partitions keep moving forwards and there histories keep on evolving in terms of container switches. In the current scenario, there are several cases that can arise and need to be handled.\\


But before we can visit the different cases, we need to look at solution that would be used to resolve the case of network partitioning. \\

\textbf{General Solution}: The descriptor that contains a subset of information about the nodes is exchanged by the nodes with others in the system. The information needs to be augmented with the \textit{Last Leader Unit} that the node has seen when exchanging the information in the system. This information will be used by the other node to check if the unit is contained in the \textit{ History} which is the sorted list of the unit updates that has been seen by that particular node. Every node maintains a local history of the leader unit updates. 

In case the leader unit is not found in the history, the node is marked as a suspected node and handed over to the application for further investigation. The case of finding a node in the leader history also needs to be expanded. Below are cases that could occur between the exchange between nodes A and B.


\begin{enumerate}

\item Both nodes have same last leader unit in there respective histories. 

\item One of the nodes is ahead in the history and the other node's last leader unit is present in the history of the former one.

\item One of the nodes is ahead in the history but it does not contain the leader unit provided by the other node in its local history.

\end{enumerate}


The steps that the node will take will vary based on the scenario that the node is operating under.\\


\textbf{General Shuffle Scenario}: Let us assume there are two nodes A and B in the system. As part of periodic shuffle round, node A wakes up and generates its own descriptor based on the latest information available to the \textit{Partition Aware Gradient}. The node picks up a node to shuffle with based on its utility. The gradient component simply sends the shuffle request to the node. The message is then intercepted by the Partition Aware Gradient Component and this component decides whether to send the request forward or not:


The partition aware gradient looks at the \textit{last leader unit} of the node in the request. Now two cases could arise :

\begin{enumerate}

\item Either the node is present in the history of the sending node and therefore the request is allowed to pass through.

\item The node is unable to find the node's last leader unit in the history and thus puts the node in the suspected list and hands over the list to the application for further investigation. (This task could also be performed by the component itself but depends upon the data needs to be accessed for investigation.)


\end{enumerate}


\textbf{Questions}: Some questions based on the above scenario:

\begin{enumerate}



\item Is there a node that is truly ahead of another node ?

\textbf{Ans)} The last leader unit that the node is at helps to determine if a node is ahead of another node but due to network partition merge there might be cases in which the nodes that are rising in the system are at the same epoch in terms of last leader units but having different leader id's in the leader unit.



\item What happens if the Last Leader Unit of the descriptor that the node is trying to shuffle with is ahead of the node's self last leader unit ?

\textbf{Ans)} It might be that the node be lagging behind on the updates and therefore haven't seen them yet or might be a case of network partition merge. So, I think it means that the \textit{Partition Aware Gradient} should only allow the nodes whose last leader unit is a subset of it's own history and in all the other cases should send it to the application for inspection. ( \textbf{ Not sure on this one } )




\item How does the Partition Aware Gradient decides what to block and what to allow (\textit{information from the request/ response and the croupier data push}) ?

\textbf{Ans)} The PAG needs to exert a tight control over the message exchange as part of Gradient shuffle and also the data coming from the croupier. The messages need to be intercepted and then the descriptor is analyzed by the PAG. In case a suspicious descriptor is found, the application is informed about the unverified node which can be a potential partition merge and therefore the PAG currently blocks it and waits for the application to come back to it about the nature of the unit update. 


\item Can the PAG direct the gradient to remove a particular sample from the Gradient ?

\textbf{Ans)} What happens to the sample in the gradient that the PAG refuses to shuffle with and drops the message. Can the PAG direct the gradient also to remove the node descriptor so that the gradient should not spread the sample to other nodes in the system ?

\item How does the case of the Sharding differs from the case of partition merge in which the nodes from one branch are at the same level and the nodes in other branch have moved forward in terms of container switches and then the partition heals ?

\textbf{Ans)} \textit{ Not sure on how to resolve this one }. Just one thing that comes to mind is that the application components of the nodes should have bulk of responsibility of deciding the nodes or better yet the leader units that are safe to shuffle with. In addition to this, the PAG check needs to be applied both at the request and the response phase of the protocol.

\item What are the steps that the Application needs to take ?

\textbf{Ans)} Once the application receives the suspected list from the Partition Aware Gradient (PAG), the application needs to determine that the node is not from other partition and it is not a case of Partition Merge. The application initiates a verification mechanism in which based on the current last leader unit, sends a verification request to the node about the last leader unit, in sense that it asks the node that : \textit{ Hey I am not able to determine whether to shuffle with you or not as you are ahead, so do you see me in your history ?}

In addition to this, what needs to happen in case the last leader unit of the node provided by the PAG is behind the current last leader unit and not in the history ?

It can be a false positive as there might be a network partition merge going on and the node is a new node which has fetched the unit update from a node that has already received the partition merge update and incorporated the new units in its history. How does the application determine it's a case of false detection of partition merge. The leader intervention is required in this case ? ( \textit{ Does the node route the query upwards towards the leader with a predefined TTL ? } )

\end{enumerate}



\section{Special Cases}


\subsection{Split Point Case} A very common scenario that could occur as part of Network Partitioning is that one of the partitioned branches (Ones that don't have the leader at which the partition occurs ), thinks that the leader is dead and therefore close the leader unit at the max entries based on the entries present in the new leader. The other branch could very well be continuing with the same leader unit and adding more entries in that unit and therefore  we have a unique case in which \textit{histories of both leader units} will contain the same leader unit but with different entries.

In case the partition heals at the point at which the common leader unit is still on going in one of the leader units, this need to be handled correctly because the nodes when asking the last leader unit from the other nodes, they will have the unit in the history but the entries in that unit will be different. This can create issues in terms of entries being pulled by a particular node. \\

\textbf{Solution}: The solution to this should be that there needs to be a checking between the current tracking id and the max entries of the leader unit being tracked. The node should always ask for the entries up-to a maximum point in the request in the closing update has been received for the leader unit. 

\subsection{Buffered Leader Unit} The case of buffered leader units is kind of tricky to understand. It is important to fully understand the premise because we need to solve it correctly. A node in the system usually buffers the updates when he is lagging in the leader unit pull in a current partition. In this case it might be possible that the node becomes a part of leader group due to churn. Therefore we might see jumps in the leader unit history which leads to the buffering of updates. A node which is simply lagging behind and pulling the updates from the nodes nearby will always get the updates in order and therefore the case of buffered updates will not occur.

In case the updates being pulled through the control mechanism fill a jump, we need to check for the next epoch updates from buffered ones and therefore apply them in order. It is because the buffered updates are important and needed as these updates might be necessary for the nodes in the current partition as the nodes that buffered them might be the only ones except leader to add them and therefore it is necessary to keep them buffered. \textit { They are necessary for the liveness condition of the system. }

While adding the buffered updates we must take care of the sharding update and empty the buffered unit list when the sharding update is successfully processed by the system. Because the nodes are concerned about the current shard and if they have buffered entries from shard ahead, it means that data is already replicated in the shard and therefore we can again pull once we have sharded. 


\subsection{Partition Aware Gradient}
The PAG needs to be constructed carefully as there might be a lot of cases in which can result in inconsistent state. Some of the considerations are as follows:

\begin{enumerate}

\item The node needs to keep a buffered list of verified and suspected nodes. The list should always be cleared when a new last leader unit is received because the node which might be a current extension based on latest leader unit, might not be for the next leader unit switch and therefore needs to be taken care of by completely emptying the verfied list. Suspected ones need not to be cleared as the node always move forward in the system and if a current node is not a logical extension then it is very likely that it will not be later onewards also.


\item In every croupier push or gradient shuffle request, we initially block the request. We first run it through the verified list and then in case it is still a suspect, based on the supplied last leader unit, we either send it to the application or to the component that tried to shuffle. We simply ask, \textit{Hey, do you think this unit is in my history or do you see my last leader unit in your history ?}. In case the response is affirmative, we add it to the verified list and buffer it. It addition to this, we also need to store the original request and trigger it to the application in case of a positive response because of following reasons:

\begin{enumerate}

\item In case we assume that by simply adding to the buffered list, next time the request comes it will simply be sent through. The assumption is flawed as the maximum size of the verified set is regulated and therefore it might happen that the next time the request comes, the information might already be cleaned.

\item In addition to this, we don't know if the node is going to send the request again. Therefore, it might result in disconnected nodes in the system.

\end{enumerate}


\item When filtering samples in the croupier as part of potential partitioned nodes in the system, we need to be careful with certain things. Gradient by default doesn't allow the nodes that are lower in terms of partition depth in the gradient. Therefore we need not care about those. We need to be careful about the nodes at the same level and the level ahead. Therefore we allow the croupier to actually send in the network partitioned samples in the gradient and the application which are below itself. Now as for the nodes that are the same level and the level above, we check for the network partitioned nodes but performing the last leader unit check.


\item In index pull mechanism, you not only indicate the entry that you are currently at but also the max entry that you need to pull to based on the current epoch closing value. This is necessary as it will prevent the nodes from pulling entries that will be eventually added to the nodes but not now. It will also create disparity between the nodes in the system in terms of uneven entries. 

\item In order to prevent the corner merge cases as stated earlier, the splitting point case, we need the max entries in the leader unit that are being added currently. Earlier, we simply added the max entries when we wanted to close the epoch. So the leader unit structure would look something like \textit{(epoch, leaderid, ONGOING, 0)} initially and then \textit{(epoch, leaderid, COMPLETED, 100)}. The leader unit didn't evolve with time. This lead to the control pull pulling the same \textsc{ONGOING} update with the 0 entries, which prevented the index pull to evolve in a structured manner and therefore the pull could simply start pulling wildly and go beyond the max entries in case of partition merge. This is one of the main reasons that we need to have current max entries in the leader unit and then the node gets the update for the next entries limit that needs to be pulled. 

This will make the nodes a little slow, it terms that the update from the control pull tells them that 10 more entries has been added, go look for them in the system.  And then the node goes and look for them only till that point. Because now when the node requests for the next entries and the other partition merge happens, they will deny him saying that \textit{Hey, you say give me next updates from mentioned epoch, leader and max entries but it seems that this epoch already closed at max entries - 10, so I can't give you anything for now, return }. 

The PAG also gets the update regarding the max entry size of the ONGOING last leader unit and will be used in the look ups. Once the node is detected as bad for shuffling the descriptor eventually becomes old and phases out.


\item Why can't we simply divide the entries at the sharding point only ? \\
Dividing the entries at the sharding point is a difficult task because then also have to remember that we also have to split the entries in shard at the point mentioned. In addition to this, the index pull tracker needs to be updated about the max entries that he is allowed to pull when in the splitting shard. This itself becomes a hack. Instead, if we split at the starting point of the shard then it will be a little easy to generate the skip list as complete leader units will be in that list. 


\end{enumerate}

\newpage
\section{Leader Election Protocol}

In context of distributed systems various leader election protocols have been discussed {several references}. Election of leader for a group of nodes in the system typically requires all the nodes to agree upon the leader. Once the leader gets elected, it coordinates between the nodes and helps the group to move forward. 
\par As mentioned above, all the nodes usually need to know about the node that would be elected as leader but in case of large scale system it would usually result in huge traffic in the system. As part of this research, we introduced a weaker form of leader election protocol known as \textit{leader selection}. As part of this protocol, we select a group of \textit{top K} nodes in the system. The node which has the highest utility among the K members tries to assert itself as the leader. The node in order to become the leader asks for promises from the other nodes in the selected group. In case, the nodes see anybody above the node rejects the promise. The requesting only moves to the commit phase after receiving promises from all the nodes in the group. In case any one of them rejects, the node resets the convergence counter and waits for the system to stabalize again. 

\par In order to better understand the mechanism, we need to look at the overlay over which the system is built. We use the Gradient topology, which is a class of P2P topology in which the nodes in the systems organize themselves in the network in such a way that the nodes with the highest utility  are concentrated at the center of the network and the nodes with low utility lies on the outer edges of the network. We use a \textit{preference function}, according to which nodes have a higher preference to form connections with the nodes which have utility higher but closer to itself with higher probability. In this way the nodes arrange themselves in the system. In addition to this, every node runs a convergence function in which it tries to calculate a percentage change in the number of neighbors. In case the change is greater than a predefined threshold \textit{D}, the node resets the convergence counter. On the other hand, if the change is below the threshold the node increments the counter. 
\par The node only tries to assert itself the leader, when the convergence counter has reached a certain value and the node based on its own utility sees no other node above it. It is then the node starts the promise round with the other K nodes. This process of determining if self node is a leader is carried out by every node in every shard in the system. Eventually the leader gets elected for every shard. In addition to leader, every shard also has a follower group selected by the leader itself. The purpose of the follower group is to help the leader with the replication of the commands and the steps taken by the leader. In case the leader dies, the next higher node usually from the follower group takes over the leadership for that shard. The main functionality of the members of the followers group is as follows:


\begin{enumerate}

\item Reject all promises in case the node has already promised or is a part of follower group.

\item Accept a promise in case the node doesn't see anybody above the requesting node in terms of utility.

\item Expire a promise in case the commit message is not received before the timeout for a specific promise.

\end{enumerate}


As part of the leader selection mechanism, we try to elect a leader as soon as possible. In a very dynamic system where nodes can join and leave the system, it is very much possible that a better node comes along and try to assert itself as a leader thus violating the safety condition of having only a single leader per shard in the system. In order to prevent this, when a node gets elected as the leader, it along with the chosen follower group, artificially augments the utility by switching on the leader memebership check. This check places the group at the center of the shard. In this way when a better node comes along, it cannot instantaneously start the election protoco because it's self membership check is switched off and it sees the current group as better than himself. 

\par This feature of artifically increasing the utility might lead to violation of fairness protocol, meaning that a node which became a leader initially, can become the leader for eternity in case the group membership check is not switched off. Therefore, it is the responsibility of the leader to locally switch off the leader group membership check and then look at the neighboring samples. In case the leader finds a better node, the leader simply backs of by terminating its membership. The follower group detects it and removes themselves from the group membership. The better node detects this and then starts the promise round itself and takes over the leadership for the current shard.



\section{Algorithm}

\begin{algorithm}[h]
\caption{Eventual Leader Selection - Leader} 
\label{leader}
\begin{algorithmic}[1]
\Upon[init]{nodeId}
  \State $selfId := nodeId;$ $round := 0;$
  \State $isLeader := false;$ $stableGradient := false;$
  \State $gradSample := \emptyset;$ $followerGroup := \emptyset;$
  \TriggerS[periodicCheck]{}\EndTriggerS
 \EndUpon

\Upon[gradientSample]{sample}
  \State $stableGradient := $ \emph{stable}$(sample);$
  \State $gradientSample := sample;$
 \EndUpon

\UponS[periodicCheck]{}
  \If{$!isLeader$ AND $stableGradient$ AND \emph{highestUtility}$(selfId, gradientSample)$}
     \State $followerGroup := $ \emph{getTopK}$(gradientSample);$
     \ForEach[followerGroup]{member}
       \Trigger[promiseReq]{selfId, round} \EndTrigger
    \EndForEach
    \TriggerS[roundTimeout]{} \EndTriggerS
    \State $promiseSet := \emptyset$
  \EndIf
 \EndUponS

\Upon[promiseAck]{pNodeId, pRound}
  \If{pRound = round}
    \State $promiseSet := promiseSet \cup \{pNodeId\}$
    \If{followerGroup = promiseSet}
      \ForEach[followerGroup]{member}
        \Trigger[lease]{selfId}\EndTrigger 
      \EndForEach
      \State $isLeader := true;$
      \TriggerS[leaseTimeout]{}\EndTriggerS
      \TriggerS[cancelRoundTimeout]{}\EndTriggerS
    \EndIf
  \EndIf
\EndUpon

\Upon[promiseNack]{pNodeId, pRound} 
  \If{pRound = round}
    \State $round++;$
    \TriggerS[cancelRoundTimeout]{}\EndTriggerS
  \EndIf
\EndUpon

\UponS[roundTimeout]{}
  \State $round++$
\EndUponS

\UponS[leaseTimeout]{} 
  \State $sample := $ \emph{resetUtility}$(gradientSample);$
  \If{\emph{highestUtility}$(sample)$}
    \State $followerGroup := $ \emph{getTopK}$(sample)$
    \ForEach[followerGroup]{member}
      \Trigger[lease]{nodeId} \EndTrigger
    \EndForEach
    \TriggerS[leaseTimeout]{}\EndTriggerS
  \Else
    \State $isLeader := false;$
  \EndIf
\EndUponS

\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Eventual Leader Selection - Follower} 
\label{follower}
\begin{algorithmic}[1]

\UponS[init]{id}
  \State $isFolower := false;$
  \State $selfId := Id;$ $pendingPromise := Nil;$
  \State $gradientSample := \emptyset;$
\EndUponS

\Upon[gradientSample]{sample}
  \State $gradientSample := sample;$
 \EndUpon

\Upon[promiseReq]{nodeId, round}
  \If{$!isFollower$ AND $pendingPromise = Nil$ AND \emph{highestUtility}$(nodeId, gradientSample)$}
    \State $pendingPromise := nodeId;$
    \Trigger[promiseAck]{selfId, round}\EndTrigger
    \TriggerS[promiseTimeout]{}\EndTriggerS
  \Else
     \Trigger[promiseNack]{selfId, round}\EndTrigger
  \EndIf
\EndUpon

\UponS[promiseTimeout]{}
  \State $pendingPromise := Nil;$
\EndUponS

\Upon[lease]{nodeId} 
  \If{$pendingPromise = nodeId $}
    \TriggerS[followerLeaseTimeout]{}\EndTriggerS
    \TriggerS[cancelPromiseTimeout]{}\EndTriggerS
    \State $isFollower := true;$
    \State \emph{setFollowerUtility}$();$
  \EndIf
\EndUpon

\UponS[promiseTimeout]{}
  \State $pendingPromise := Nil;$
\EndUponS

\UponS[leaseTimeout]{} 
  \State $isFollower := false;$
  \State \emph{resetFollowerUtility}$();$
\EndUponS

\end{algorithmic}
\end{algorithm}



\section{Network Partitioning}
In this section we will talk about the concepts that will be used as part of the network partitioning.




\section{Abstract}
Full text decentralized search based on peer to peer architecture is still one of major unresolved problem as part of Distributed Systems domain. Traditional search engines like Google employ self owned servers to store and locate data. As part of the thesis, we propose full text decentralized system Sweep which is built on Apache Lucene.
\par The approach uses a gossiping protocol to discover new peers in the system. In addition to this, every node connects to a subset of nodes in the whole system based on there utility or usefulness to that particular node. Load balancing in the system is achieved by automatically sharding the entries when the corresponding limit has been reached. Each shard has a leader of its own which the responsibility of taking decisions like addition, deletion and updationg of entries, splitting in event of shard reaching its predefined capacity.




\section{Introduction}

\subsection{Motivation}
Motivation Behind the thesis.

\subsection{Delimitations}
Boundaries of the approach.

\subsection{Contribution}
The main aim of the thesis is to come up with up with a approach for creating a Partition Aware full text decentralized search engine. The main contribution of the thesis is as follos:

\begin{itemize}
	\item Design and implementation of a full text decentralized search engine.
	\item Design and implementation of a weaker form of leader election protocol to elect a trusted node for a particular shard which would be responsible for implementing the decisions like addition, deletion and updation of entries in the system.
	\item Designing a basic approach for the detection of the network partition and merge during the partition healing.
		
\end{itemize}

\subsection{Outline}
The main outline of the thesis. Main chapters as part of thesis.



\section{Background}
In background section, we provide explanation about the technologies and concepts used. Here we talk about the \textbf{Google Search, Git Merge, Kompics, Apache Lucene, Consistency Models and CAP Theorm.}


\section{Related Work}

\begin{enumerate}
	\item Amazon Dynamo
	\item Cassandra
	\item ElasticSearch and Solr
\end{enumerate}


\newpage
\section{Design}


\subsection{System Model}

\begin{itemize}

\item \textit{Peers}: The system is composed of peers which basically are processes executed by the clients. The clients join the system by bootstrapping themselves with the already present peers in the system. The peers follow the Crash/ Stop process [ref] i.e a peer upon failure crashes and stops all communication with the other nodes. Once the node has failed that node never comes up. 


\item \textit{Communication Links}: As part of system design we assume the communication link to be fair-loss links[reference]. Therefore, the network only transfers the messages that are sent from the application and cannot create the messages on its own. In addition to this, we assume the network to be partially synchronous meaning there are times during which the network might being asynchronous and drop messages but eventually the network will become synchronous with upper bounds on the message transmission time.

\item \textit{Overlay Network}: The system is built upon Gradient [reference] which constructs an overlay based on the preference function. The preference function used for the application will be explained later in detail but in general it helps to identify the ranking of node as compared to others. The Gradient in turn is built upon a peer sampling service which provides random samples from the application to the Gradient component.

\end{itemize}



\subsection{Overlay Network}




\subsection{Architecture}

\subsubsection*{Preference and Utility Function}

Each peer in the system has an associated utility based on the state of the peer at that instant. As the system evolves with time, so does its utility. As already mentioned, in order to build the overlay network, system uses a \textit {Preference Function} based on the utilities of the nodes. A peer uses this preference function mainly during the selection of peer for shuffling and to keep track of preferred neighbors. The preference function in case of  Sweep is selection of the peers which have a higher utility than self but closer to self with higher probability. As seen in the diagram, if we assume utility having an absolute value, as per the preference function, \textit{Peer C} will prefer \textit{Peer A} over \textit{Peer B}, even though both have higher utility than C because A's utility is closer to utility of A.

\par The preference function is based on the utility of a particular peer. It helps a peer to select a preferred peer from a collection of peers. The utility function is generally any metric or set of rules usually depending upon the application requirement which would help to create a total order on the peers in the system. In case of Sweep, the utility function is as follows:

\small 
\begin{equation*}
    LeaderGroup \prec ReplicationScore \prec PeerScore \prec PeerID
\end{equation*}
\normalsize

In process of comparison of utility values between two different nodes, we simply compare them in order defined above i.e from \textit {left to right}. We move on to the next parameter in the utility function in case ordering cannot be achieved on the previous parameter. The utility function comparison starts by looking at the leader group membership check which indicates if a node is part of leader group for a particular shard. In case a node has this check switched on, the utility of the node suddenly increases compared to the other peers in the same shard. The Replication Score parameter indicates the quantity of the entries that a peer has its local lucene store. As system evolves and more entries are added in the shard the peer with a higher \textit{Replication Score} count indicates a better node in terms of serving latest entries added. Keeping in view the fairness protocol, we also have a \textit{PeerScore} metric in the utility function. This score helps to identify the peers with superior resources in terms of computational capability, higher bandwidth. These nodes are given importance as they can easily serve a larger audience in the shard. The peer id is used a tie breaker in the function. We assume that the peers joining the system have unique peer id's.  In this way, we achieve a total order on the peers in the system.

\par The benefit of achieveing total order on the nodes is the ability to assign ranks to the nodes with the lowest rank i.e. 0 being assigned to the \textit{leader} of the shard. As the system is continously evolving, the utility of peers is changing and therefore the peers in the shard based on the neighboring peers can then adjust there own ranks. The purpose of assigning ranks is the creation of fingers which are nothing but long range links that each peer hold. Based on the rank, the peer usually try to find fingers i.e. links to the peers that are half its own rank. These fingers help speeding up the dissemination of the updates in the system.


\subsubsection*{Pull Protocol}

The general overall functioning of the system involves electing a leader i.e a node with the highest utility at that instant of time as the leader of shard. The leader takes decisions in the shard in terms of addition/ deletion and updating entries in the shard. In order to prevent a single point of failure the leader performs a two phase commit to the nodes elected as part of leader group. The Leader Election protocol is explained later in detail in section [mention section]. Once the entry has been replicated to the leader group nodes, the leader returns back a successful response to the client. Now the peers can either push the entries to the neighboring nodes or nodes need to pull the entries added. The push protocol seems lucrative in terms that the peers as soon as they discover about a new entry, push the entry to other nodes. It prevents the other nodes constantly looking for the entries by asking the higher utility nodes. But there are major flaws in this scheme.

\begin{enumerate}

\item First, we cannot guarantee that the entry will be received by every node in the shard as it is based on push protocol and current neighbors. In case a particular peers reference is not in the higher utility nodes, the entry will not be pushed to it.

\item It makes the system vulnerable to Byzantine Nodes [reference] which can easily push junk data.

\item The biggest flaw in this protocol is way in which the preference function is defined. As mentioned earlier a particular node prefers node which higher but closer utility. This means that given enough peers which have higher utility, the peers neighbors will all point to the nodes above in the Gradient, therefore the nodes which are on the edges of a particular shard will never receive the entry.

\end{enumerate} 

As a result, we introduced pull protocol, in which the nodes constantly request the peers higher than self in terms of utility about any latest developments or decisions taken in the shard by the leader. This way there are able to disseminate the information in the system. The dissemination protocol will be explained in greater detail in section [mention section]



\subsubsection*{Gossip Based Data Dissemination}
Once a peer becomes a leader, it is implicitly trusted by other members in the shard.
Initially the information of a node becoming a leader is only published at the leader group chosen by the leader. This information then gets pulled by other peers in the system. In this situation, it might be very much possible that a Byzantine Node might start publishing wrong information to the peers during the pull protocol. In order to avoid such situations, a peer during the pull protocol, requests updates from a predefined configurable peers usually three that are above itself in terms of utility.  The peers respond with the hashes of the information. Only the information for which the hashes match from all the nodes is then pulled from anyone of the responding peers. 
\par This approach is being used for securely disseminating the leader information in the system. In addition to this, entry dissemination is also based on this protocol. We think that this approach is secure enough in the sense that it is highly unlikely for all the nodes that are requested by peer to provide updates to be faulty.


\subsubsection*{Load Balancing}

As the system evolves, the leader receives request for adding new entries in the system. Using basic replication strategies the leader commits the entries to the leader group which then gets replicated to all the nodes in the shard using pull protocol. The system has a predefined \textit{MaxShardSize} parameter. When the entries grow beyond the count, the leader then initiates \textit{ShardSplit} protocol. As part of this protocol, each peer based on its identifier and its depth which is basically number of shard splits seen by the node, decides the next shard id. 
\par Before initiating the two phase commit of the shard split, the leader stops handling the entry additions. It then looks in its local lucene store and after sorting the entries based on there identifiers, calculates the split point. This meta information is then committed to the leader group nodes along with the shard split update as a part of two phase commit. Once the information is pulled by other peers in the system, based on there new shardId, they decide to remove all the entries either before or after the split point. The peers then join the other peers in there respective shards. 
\par It might be argued that after the shard split, the entries get divided, so the search results might not be complete. It order to prevent this, each peer has a \textit{routing table} which keeps references to the peers in other shards. The table gets populated and updated by our \textit{Peer Sampling Service} which provides peers with random samples from different shards. Upon receiving shard request, the peer simply forwards the request to the peers in the other shards.

\begin{itemize}

\item Application Entries 

\end{itemize}


\newpage
\section{Evaluations}


In this section we will be writing about the various experiments that were performed.

\begin{enumerate}

\item Mention about the machine used to perform simulations.
\item Mention the man internal parameters that were set during the simulations.
\item Then mention the experiments. Explain the experiment with the see-saw version.


\end{enumerate}





\subsection{Experimental Setup}

The evaluations were performed on the machine with the following specifications:

\begin{itemize}

\item 8GB of RAM
\item Eight Cores (Intel(R) Core(TM) i7 CPU 860  @ 2.80GHz)
\end{itemize}

The nodes were started in simulation on a single machine. A helper module was used to keep track of all the nodes started in simulation in order to communicate with them during the entry addition phase. A special node known as the leader node was also started with the lowest value of the identifier. Keeping other parameters as same, this allowed the node to become leader during the initial leader election phase. In addition to this, a global aggregator node was also booted. The peers internally have a local aggregator component which keeps track of the overall state of the peer based on the information received from various other components. The local aggregator constantly sends the information over to the global aggregator node. In this we are able to keep track of the overall state of the system.



\subsection{Workload Setup}

Unless otherwise specifically mentioned, we ran simulations with cluster of 200 nodes. The simulation in Kompics is single threaded in order to reproduce the results, the limiting factor was the capacity of single core of the machine. The other parameters that were tuned for the simulations are as follows:

\begin{itemize}
\setlength\itemsep{0em}
\item \textit{gradientViewSize} - 10 nodes
\item \textit{croupierViewSize} - 10 nodes
\item \textit{gradientShuffle} - 1000 (ms)
\item \textit{croupierShuffle} - 1000 (ms)
\item \textit{branchingFactor} - 10
\item \textit{electionConvergenceRounds} - 6
\item \textit{electionConvergenceTest} - 0.8d
\item \textit{Entry Pull Round} - 4000 (ms)
\item \textit{Control Pull Round} - 3000 (ms)
\item \textit{maxEntryExchange} - 25 nodes/ pull
\end{itemize}

In addition to it, the minimum nodes that nodes needs to pull either control or the entry data needs to be \textbf{three}. This conditions is respected by each and every node unless they find the leader in the view of there own gradient. Every node implicitly trusts the leader and therefore pull data directly from it.




\subsection{Convergence Experiment}

The \textit{Convergence Experiment} helps to understands the rapidness with which the entry gets disseminated in the system. As part of this experiment, we booted up 250 nodes in simulation. Once the system was stabilized in terms of leader of the shard getting elected and the marker entry for the current epoch was already added to each node, we then added a single entry to the leader and analyzed the time it took for the entry to be disseminated to all the nodes in the system. The results as shown in \textbf{Figure} points out that it took 8 - 9 (sec) for all the nodes to get the entry. This time evaluates to roughly two pull rounds based on the current pull round timeout.


The Figure 2 shows the same experiment but now with 500 nodes in the simulation. The outcome is same as the previous one. The reason is because of the \textit{branchingFactor} parameter which is set to 10. Because of this, the nodes in the range of 60-600 are supposed to get the entry in approx 2 rounds. The results also conform with the expected outcome. 


\subsection{Add Entry Throughput}

In this experiment, we initially booted up 200 nodes in simulation. Once the gradient and the leader group becomes stabilized, initial batch of 20 entries were added in the system. We then started adding entries at varying rates likes \textit{1 entry per sec}, \textit{2 entry per sec} and  \textit{6 entry per sec} for a fixed predefined period of time. As part of this experiment, we analyzed the fifty, seventy five and ninety nine percentile values i.e. number of nodes which at any given time during the operation of entry addition have fifty percent, seventy five percent and ninety nine percent of the entries. 

\par The Figure 3 depicts the metrics in case of \textit{1 entry per sec}. As we can see the fifty percentile metrics seems stable as in all the nodes during the entry addition phase has atleast fifty percent or more of the entries. In case of seventy five percentile, it depicts a seesaw like structure. This behavior of the system initially looks strange but a closer look at the interaction of the application with the overlay over which the system is built can help to understand this behavior. As mentioned earlier, a node in order to perform a control pull needs to have atleast three nodes above himself in terms of utility. The node then matches the responses and incorporate the common in order responses in the local \textit{timeline}. As node mainly fetches the leader unit updates through the control pull, the entry pull mechanism takes this updated information about the number of entries in the system, requests for the next entries from the nodes above it. The mechanism of matching the responses is same as in case of control pull protocol. 
\par The node then incorporates the entries by adding them to the Lucene Store and augmenting the utility accordingly. The application then informs the other components about its updated utility which allows them to make informed decisions. The overlay network contains the sample of nodes based on the preference function. With a sudden increase in the utility the local view of the gradient becomes obsolete and therefore the number higher samples return by the gradient to the application diminish. In case a node unable to find desired number of higher utility nodes for the control pull simply returns. It is because of the obsolete gradient samples the node doesn't start the control pull. In absence of the updated information about the entries in the system, the entry pull mechanism is unable to pull the next entries. Therefore the nodes in range of seventy percentile drops. Once the sample gets refreshed due to continuous shuffling and nodes injecting updated utilities in the system, the nodes are able to find the nodes above itself and start pulling the control information  resulting in the new entries being pulled.

\par The Figure 4 depicts the performance of the system in event of addition of \textit{6 entry per sec}. The seesaw like structure can now be observed both in the fifty and seventy-five percentile. The reason being same as mentioned above. As the nodes near to the center of the gradient waits for the gradient to stabilize to get updated information, the nodes farther away near to the edges of the gradient are also unable to get the updated information. Thus they also start lagging in terms of catching up to the leader group with has the latest information in terms of entries being added in the system. Apart from this, as observed from the diagrams the ninety percentile is really low in range of \textit{1-3\%}. The reason for this is the entries present in the system. As the entries that are added are usually low in addition to the initial entries, even a difference of one entry will remove a particular node from the ninety percentile. Therefore only the leader group nodes to which the leader replicates the entry are present in this range.






\subsection{Churn Experiment}

As part of this experiment, we initially started the system with 200 nodes and loaded the system with 100 entries. We started with the addition and the simultaneous removal \textit{4 Nodes per sec} of the node from the system. We continued this behavior for \textit{60 Seconds} thus resulting in a churn of \textit{4 \%}. Nodes from the system except the nodes from the leader group were selected randomly to be removed by stopping there execution. New nodes were booted up with different node ids. In addition to this, a constant rate of \textit{1 entry per sec} was employed for the entry addition. Figure 5 depicts the fifty, seventy five and the ninety percentile values. As we can see more than fifty percent of the nodes had fifty and seventy percent of the entries at all times during the experiment. Only the leader group nodes seemed to be in the category of nodes having ninety nine percent of the entries at all times. 



\subsection{Flash Crowd Experiment}
As part of this experiment we started with 200 nodes in the system and loaded the system initially with 100 entries. Then we introduced bunch of fresh nodes in the system simultaneously and analyzed the time it took for the nodes to catch up. We performed the experiment for flash crowd size of \textit{10 percent, 20 percent and 40 percent} respectively. As seem from Figure 6 and 7, it took \textit{20 seconds approximately} for the nodes to catch up with fifty percent of the entries. In addition to this, the time taken by nodes to catch up with the seventy five and ninety nine of the entries is approximately the same. 

\par Ideally as the \textit{maxEntryFetch} parameter per entry pull is \textit{twenty five}, it should take sixteen seconds i.e \textit{4 pull rounds} approximately for the nodes to fetch all the entries in the system but as observed from the diagram it takes more time than that. It is because it takes some time for the nodes to adjust themselves in the overall gradient and then pull the control information. Once the control information is pulled and verified, the nodes are able to determine the entries in the system at that moment and then start looking for the entries by requesting them from the nodes higher than themselves in terms of utility.
































\end{document}

