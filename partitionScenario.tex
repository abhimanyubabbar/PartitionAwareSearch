\documentclass[a4paper, 11pt]{article}
%\usepackage[margin=0.8in]{geometry}
%\usepackage[T1]{fontenc}
%\usepackage{babel}
%\setcounter{section}{6}
\usepackage{times,amsmath,epsfig}
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames]{color}
\usepackage{amssymb}

\let\labelindent\relax

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{pdfpages}
\usepackage{fancyhdr}
\usepackage{exscale}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{ntheorem}
\usepackage{paralist}
\usepackage{verbatim}
\usepackage{color}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{listings}      % source code
\usepackage{algorithm}
\usepackage{algorithmicx}  % pseudo code (1/2)
\usepackage[noend]{algpseudocode} % pseudo code (2/2)
\usepackage{setspace}
\usepackage{tikz}
\usepackage{array}
\usepackage{relsize}
\usepackage{diagbox}
\usepackage{listings}


\DeclareMathOperator{\union}{\mathtt{union}}
\providecommand{\myfloor}[1]{\left \lfloor #1 \right \rfloor}

%%%%%%TRIGGER
\algblockdefx[TriggerS]{TriggerS}{EndTriggerS}[2][event]
  {\textbf{trigger} $\langle #1 \rangle$;}

%No extra line - TriggerS 
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndTriggerS}}
  {}%
\makeatother

\algblockdefx[Trigger]{Trigger}{EndTrigger}[2][event]
  {\textbf{trigger} $\langle#1$ $\mid$ $#2\rangle$;}

%No extra line - Trigger  
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndTrigger}}
  {}%
\makeatother

%%%%%%FOR EACH
\algblockdefx[ForEach]{ForEach}{EndForEach}[2][collection]
   {\textbf{ for each } $#1$ \textbf{as} $#2$}

%No extra line - ForEach
\makeatletter
\ifthenelse{\equal{\ALG@noend}{t}}%
  {\algtext*{EndForEach}}
  {}%
\makeatother

%%%%%%UPON
\algblockdefx[UponEvent]{Upon}{EndUpon}[2][event]
  {\textbf{upon event} $\langle#1$ $\mid$ \emph{#2}$\rangle$ \textbf{do} }

\algblockdefx[UponEventS]{UponS}{EndUponS}[2][event]
  {\textbf{upon event} $\langle#1\rangle$ \textbf{do} }



\author{
  Babbar, Abhimanyu\\
  \textsc{890729-7751}\\
  \texttt{babbar@kth.se}
}
\title{Network Partition Scenarios} 

\begin{document}

\maketitle 

\section{Scenarios}
Analysis of the different Network Partition Scenarios and there potential solutions.

\subsection{Very Short Lived Partition}
The basic and a very frequent scenario that would occur would be that for a very short duration of time, a set of nodes gets networked partitioned. The duration is small enough so that the nodes that got partitioned \textbf{are unable to elect a new leader}.\\

\textbf{Solution}: The nodes simply merge back without any hiccups and start working as before and fetch the entries if any that could have been added in that short duration.


\subsection{Short Lived Partition}
A scenario in which the nodes partition in separate branches and diverge from a common point. The stale descriptors for some time will be present in the gradient and might be buffered in the communication maps of the application. In this, it might be possible that the other partition moves ahead and its leader unit history evolves but the partition heals before the application in the other branch is able to clean the partitioned nodes. \\

\textbf{Solution}: The solution for this case will be tricky because the nodes will still contain stale samples from other nodes in the system and in case the partition heals without the samples being removed from the node the node will contact with the nodes. Therefore more tight control needs to be imposed on every message exchange in terms of introducing last leader unit in every exchange to prevent them to serve the data which does not belong to the current network partition. It will pollute the partition.

\subsection{Medium Lived Partition without External Intervention}
Scenario in which the nodes partition in separate branches but the time for which the nodes partition is large enough so that the samples became stale enough to be removed from the gradient. As gradient implements an age threshold mechanism which simply removes all the samples from the gradient which are above a certain age. In addition to this, we assume that the stale samples are present in croupier and therefore when the partition merges back in the croupier component communicates with the samples and when response is received, the samples start getting pushed to the gradient with low ages. The gradient then filters these samples and inform application about the partitioned nodes.

\subsection{Medium Lived Partitioned With 3rd Party Intervention}
In this scenario, \textit{caracal} is used as an arbitrator and nodes constantly ping the caracal for checking for the network partition. The system when getting network partitioned, one branch of the partitioned is unable to ping the caracal and detect it as a network partition. When assume that the stale descriptors are removed from application, gradient and croupier. Therefore on partition merge, the croupier needs to be bootstrapped with the nodes from other partition and this is place where caracal is useful as it provides the nodes from the other partition on heartbeat response or separately when asked explicitly.



\subsection{General Network Partition Without Sharding}
The main case that needs to be handled as part of thesis scope is that nodes gets networked partitioned for a medium to long duration of time but the partition heals before any set of nodes reaches the sharding stage. The set of nodes act independently and the nodes which got separated from the leader, thinks that the leader is dead and therefore elects a new leader. Both the partitions keep moving forwards and there histories keep on evolving in terms of container switches. In the current scenario, there are several cases that can arise and need to be handled.\\


But before we can visit the different cases, we need to look at solution that would be used to resolve the case of network partitioning. \\

\textbf{General Solution}: The descriptor that contains a subset of information about the nodes is exchanged by the nodes with others in the system. The information needs to be augmented with the \textit{Last Leader Unit} that the node has seen when exchanging the information in the system. This information will be used by the other node to check if the unit is contained in the \textit{ History} which is the sorted list of the unit updates that has been seen by that particular node. Every node maintains a local history of the leader unit updates. 

In case the leader unit is not found in the history, the node is marked as a suspected node and handed over to the application for further investigation. The case of finding a node in the leader history also needs to be expanded. Below are cases that could occur between the exchange between nodes A and B.


\begin{enumerate}

\item Both nodes have same last leader unit in there respective histories. 

\item One of the nodes is ahead in the history and the other node's last leader unit is present in the history of the former one.

\item One of the nodes is ahead in the history but it does not contain the leader unit provided by the other node in its local history.

\end{enumerate}


The steps that the node will take will vary based on the scenario that the node is operating under.\\


\textbf{General Shuffle Scenario}: Let us assume there are two nodes A and B in the system. As part of periodic shuffle round, node A wakes up and generates its own descriptor based on the latest information available to the \textit{Partition Aware Gradient}. The node picks up a node to shuffle with based on its utility. The gradient component simply sends the shuffle request to the node. The message is then intercepted by the Partition Aware Gradient Component and this component decides whether to send the request forward or not:


The partition aware gradient looks at the \textit{last leader unit} of the node in the request. Now two cases could arise :

\begin{enumerate}

\item Either the node is present in the history of the sending node and therefore the request is allowed to pass through.

\item The node is unable to find the node's last leader unit in the history and thus puts the node in the suspected list and hands over the list to the application for further investigation. (This task could also be performed by the component itself but depends upon the data needs to be accessed for investigation.)


\end{enumerate}


\textbf{Questions}: Some questions based on the above scenario:

\begin{enumerate}



\item Is there a node that is truly ahead of another node ?

\textbf{Ans)} The last leader unit that the node is at helps to determine if a node is ahead of another node but due to network partition merge there might be cases in which the nodes that are rising in the system are at the same epoch in terms of last leader units but having different leader id's in the leader unit.



\item What happens if the Last Leader Unit of the descriptor that the node is trying to shuffle with is ahead of the node's self last leader unit ?

\textbf{Ans)} It might be that the node be lagging behind on the updates and therefore haven't seen them yet or might be a case of network partition merge. So, I think it means that the \textit{Partition Aware Gradient} should only allow the nodes whose last leader unit is a subset of it's own history and in all the other cases should send it to the application for inspection. ( \textbf{ Not sure on this one } )




\item How does the Partition Aware Gradient decides what to block and what to allow (\textit{information from the request/ response and the croupier data push}) ?

\textbf{Ans)} The PAG needs to exert a tight control over the message exchange as part of Gradient shuffle and also the data coming from the croupier. The messages need to be intercepted and then the descriptor is analyzed by the PAG. In case a suspicious descriptor is found, the application is informed about the unverified node which can be a potential partition merge and therefore the PAG currently blocks it and waits for the application to come back to it about the nature of the unit update. 


\item Can the PAG direct the gradient to remove a particular sample from the Gradient ?

\textbf{Ans)} What happens to the sample in the gradient that the PAG refuses to shuffle with and drops the message. Can the PAG direct the gradient also to remove the node descriptor so that the gradient should not spread the sample to other nodes in the system ?

\item How does the case of the Sharding differs from the case of partition merge in which the nodes from one branch are at the same level and the nodes in other branch have moved forward in terms of container switches and then the partition heals ?

\textbf{Ans)} \textit{ Not sure on how to resolve this one }. Just one thing that comes to mind is that the application components of the nodes should have bulk of responsibility of deciding the nodes or better yet the leader units that are safe to shuffle with. In addition to this, the PAG check needs to be applied both at the request and the response phase of the protocol.

\item What are the steps that the Application needs to take ?

\textbf{Ans)} Once the application receives the suspected list from the Partition Aware Gradient (PAG), the application needs to determine that the node is not from other partition and it is not a case of Partition Merge. The application initiates a verification mechanism in which based on the current last leader unit, sends a verification request to the node about the last leader unit, in sense that it asks the node that : \textit{ Hey I am not able to determine whether to shuffle with you or not as you are ahead, so do you see me in your history ?}

In addition to this, what needs to happen in case the last leader unit of the node provided by the PAG is behind the current last leader unit and not in the history ?

It can be a false positive as there might be a network partition merge going on and the node is a new node which has fetched the unit update from a node that has already received the partition merge update and incorporated the new units in its history. How does the application determine it's a case of false detection of partition merge. The leader intervention is required in this case ? ( \textit{ Does the node route the query upwards towards the leader with a predefined TTL ? } )

\end{enumerate}



\section{Special Cases}


\subsection{Split Point Case} A very common scenario that could occur as part of Network Partitioning is that one of the partitioned branches (Ones that don't have the leader at which the partition occurs ), thinks that the leader is dead and therefore close the leader unit at the max entries based on the entries present in the new leader. The other branch could very well be continuing with the same leader unit and adding more entries in that unit and therefore  we have a unique case in which \textit{histories of both leader units} will contain the same leader unit but with different entries.

In case the partition heals at the point at which the common leader unit is still on going in one of the leader units, this need to be handled correctly because the nodes when asking the last leader unit from the other nodes, they will have the unit in the history but the entries in that unit will be different. This can create issues in terms of entries being pulled by a particular node. \\

\textbf{Solution}: The solution to this should be that there needs to be a checking between the current tracking id and the max entries of the leader unit being tracked. The node should always ask for the entries up-to a maximum point in the request in the closing update has been received for the leader unit. 

\subsection{Buffered Leader Unit} The case of buffered leader units is kind of tricky to understand. It is important to fully understand the premise because we need to solve it correctly. A node in the system usually buffers the updates when he is lagging in the leader unit pull in a current partition. In this case it might be possible that the node becomes a part of leader group due to churn. Therefore we might see jumps in the leader unit history which leads to the buffering of updates. A node which is simply lagging behind and pulling the updates from the nodes nearby will always get the updates in order and therefore the case of buffered updates will not occur.

In case the updates being pulled through the control mechanism fill a jump, we need to check for the next epoch updates from buffered ones and therefore apply them in order. It is because the buffered updates are important and needed as these updates might be necessary for the nodes in the current partition as the nodes that buffered them might be the only ones except leader to add them and therefore it is necessary to keep them buffered. \textit { They are necessary for the liveness condition of the system. }

While adding the buffered updates we must take care of the sharding update and empty the buffered unit list when the sharding update is successfully processed by the system. Because the nodes are concerned about the current shard and if they have buffered entries from shard ahead, it means that data is already replicated in the shard and therefore we can again pull once we have sharded. 


\subsection{Partition Aware Gradient}
The PAG needs to be constructed carefully as there might be a lot of cases in which can result in inconsistent state. Some of the considerations are as follows:

\begin{enumerate}

\item The node needs to keep a buffered list of verified and suspected nodes. The list should always be cleared when a new last leader unit is received because the node which might be a current extension based on latest leader unit, might not be for the next leader unit switch and therefore needs to be taken care of by completely emptying the verfied list. Suspected ones need not to be cleared as the node always move forward in the system and if a current node is not a logical extension then it is very likely that it will not be later onewards also.


\item In every croupier push or gradient shuffle request, we initially block the request. We first run it through the verified list and then in case it is still a suspect, based on the supplied last leader unit, we either send it to the application or to the component that tried to shuffle. We simply ask, \textit{Hey, do you think this unit is in my history or do you see my last leader unit in your history ?}. In case the response is affirmative, we add it to the verified list and buffer it. It addition to this, we also need to store the original request and trigger it to the application in case of a positive response because of following reasons:

\begin{enumerate}

\item In case we assume that by simply adding to the buffered list, next time the request comes it will simply be sent through. The assumption is flawed as the maximum size of the verified set is regulated and therefore it might happen that the next time the request comes, the information might already be cleaned.

\item In addition to this, we don't know if the node is going to send the request again. Therefore, it might result in disconnected nodes in the system.

\end{enumerate}


\item When filtering samples in the croupier as part of potential partitioned nodes in the system, we need to be careful with certain things. Gradient by default doesn't allow the nodes that are lower in terms of partition depth in the gradient. Therefore we need not care about those. We need to be careful about the nodes at the same level and the level ahead. Therefore we allow the croupier to actually send in the network partitioned samples in the gradient and the application which are below itself. Now as for the nodes that are the same level and the level above, we check for the network partitioned nodes but performing the last leader unit check.


\item In index pull mechanism, you not only indicate the entry that you are currently at but also the max entry that you need to pull to based on the current epoch closing value. This is necessary as it will prevent the nodes from pulling entries that will be eventually added to the nodes but not now. It will also create disparity between the nodes in the system in terms of uneven entries. 

\item In order to prevent the corner merge cases as stated earlier, the splitting point case, we need the max entries in the leader unit that are being added currently. Earlier, we simply added the max entries when we wanted to close the epoch. So the leader unit structure would look something like \textit{(epoch, leaderid, ONGOING, 0)} initially and then \textit{(epoch, leaderid, COMPLETED, 100)}. The leader unit didn't evolve with time. This lead to the control pull pulling the same \textsc{ONGOING} update with the 0 entries, which prevented the index pull to evolve in a structured manner and therefore the pull could simply start pulling wildly and go beyond the max entries in case of partition merge. This is one of the main reasons that we need to have current max entries in the leader unit and then the node gets the update for the next entries limit that needs to be pulled. 

This will make the nodes a little slow, it terms that the update from the control pull tells them that 10 more entries has been added, go look for them in the system.  And then the node goes and look for them only till that point. Because now when the node requests for the next entries and the other partition merge happens, they will deny him saying that \textit{Hey, you say give me next updates from mentioned epoch, leader and max entries but it seems that this epoch already closed at max entries - 10, so I can't give you anything for now, return }. 

The PAG also gets the update regarding the max entry size of the ONGOING last leader unit and will be used in the look ups. Once the node is detected as bad for shuffling the descriptor eventually becomes old and phases out.

\end{enumerate}

\newpage
\section{Leader Election Protocol}

In context of distributed systems various leader election protocols have been discussed {several references}. Election of leader for a group of nodes in the system typically requires all the nodes to agree upon the leader. Once the leader gets elected, it coordinates between the nodes and helps the group to move forward. 
\par As mentioned above, all the nodes usually need to know about the node that would be elected as leader but in case of large scale system it would usually result in huge traffic in the system. As part of this research, we introduced a weaker form of leader election protocol known as \textit{leader selection}. As part of this protocol, we select a group of \textit{top K} nodes in the system. The node which has the highest utility among the K members tries to assert itself as the leader. The node in order to become the leader asks for promises from the other nodes in the selected group. In case, the nodes see anybody above the node rejects the promise. The requesting only moves to the commit phase after receiving promises from all the nodes in the group. In case any one of them rejects, the node resets the convergence counter and waits for the system to stabalize again. 

\par In order to better understand the mechanism, we need to look at the overlay over which the system is built. We use the Gradient topology, which is a class of P2P topology in which the nodes in the systems organize themselves in the network in such a way that the nodes with the highest utility  are concentrated at the center of the network and the nodes with low utility lies on the outer edges of the network. We use a \textit{preference function}, according to which nodes have a higher preference to form connections with the nodes which have utility higher but closer to itself with higher probability. In this way the nodes arrange themselves in the system. In addition to this, every node runs a convergence function in which it tries to calculate a percentage change in the number of neighbors. In case the change is greater than a predefined threshold \textit{D}, the node resets the convergence counter. On the other hand, if the change is below the threshold the node increments the counter. 
\par The node only tries to assert itself the leader, when the convergence counter has reached a certain value and the node based on its own utility sees no other node above it. It is then the node starts the promise round with the other K nodes. This process of determining if self node is a leader is carried out by every node in every shard in the system. Eventually the leader gets elected for every shard. In addition to leader, every shard also has a follower group selected by the leader itself. The purpose of the follower group is to help the leader with the replication of the commands and the steps taken by the leader. In case the leader dies, the next higher node usually from the follower group takes over the leadership for that shard. The main functionality of the members of the followers group is as follows:


\begin{enumerate}

\item Reject all promises in case the node has already promised or is a part of follower group.

\item Accept a promise in case the node doesn't see anybody above the requesting node in terms of utility.

\item Expire a promise in case the commit message is not received before the timeout for a specific promise.

\end{enumerate}


As part of the leader selection mechanism, we try to elect a leader as soon as possible. In a very dynamic system where nodes can join and leave the system, it is very much possible that a better node comes along and try to assert itself as a leader thus violating the safety condition of having only a single leader per shard in the system. In order to prevent this, when a node gets elected as the leader, it along with the chosen follower group, artificially augments the utility by switching on the leader memebership check. This check places the group at the center of the shard. In this way when a better node comes along, it cannot instantaneously start the election protoco because it's self membership check is switched off and it sees the current group as better than himself. 

\par This feature of artifically increasing the utility might lead to violation of fairness protocol, meaning that a node which became a leader initially, can become the leader for eternity in case the group membership check is not switched off. Therefore, it is the responsibility of the leader to locally switch off the leader group membership check and then look at the neighboring samples. In case the leader finds a better node, the leader simply backs of by terminating its membership. The follower group detects it and removes themselves from the group membership. The better node detects this and then starts the promise round itself and takes over the leadership for the current shard.



\section{Algorithm}

\begin{algorithm}[h]
\caption{Eventual Leader Selection - Leader} 
\label{leader}
\begin{algorithmic}[1]
\Upon[init]{nodeId}
  \State $selfId := nodeId;$ $round := 0;$
  \State $isLeader := false;$ $stableGradient := false;$
  \State $gradSample := \emptyset;$ $followerGroup := \emptyset;$
  \TriggerS[periodicCheck]{}\EndTriggerS
 \EndUpon

\Upon[gradientSample]{sample}
  \State $stableGradient := $ \emph{stable}$(sample);$
  \State $gradientSample := sample;$
 \EndUpon

\UponS[periodicCheck]{}
  \If{$!isLeader$ AND $stableGradient$ AND \emph{highestUtility}$(selfId, gradientSample)$}
     \State $followerGroup := $ \emph{getTopK}$(gradientSample);$
     \ForEach[followerGroup]{member}
       \Trigger[promiseReq]{selfId, round} \EndTrigger
    \EndForEach
    \TriggerS[roundTimeout]{} \EndTriggerS
    \State $promiseSet := \emptyset$
  \EndIf
 \EndUponS

\Upon[promiseAck]{pNodeId, pRound}
  \If{pRound = round}
    \State $promiseSet := promiseSet \cup \{pNodeId\}$
    \If{followerGroup = promiseSet}
      \ForEach[followerGroup]{member}
        \Trigger[lease]{selfId}\EndTrigger 
      \EndForEach
      \State $isLeader := true;$
      \TriggerS[leaseTimeout]{}\EndTriggerS
      \TriggerS[cancelRoundTimeout]{}\EndTriggerS
    \EndIf
  \EndIf
\EndUpon

\Upon[promiseNack]{pNodeId, pRound} 
  \If{pRound = round}
    \State $round++;$
    \TriggerS[cancelRoundTimeout]{}\EndTriggerS
  \EndIf
\EndUpon

\UponS[roundTimeout]{}
  \State $round++$
\EndUponS

\UponS[leaseTimeout]{} 
  \State $sample := $ \emph{resetUtility}$(gradientSample);$
  \If{\emph{highestUtility}$(sample)$}
    \State $followerGroup := $ \emph{getTopK}$(sample)$
    \ForEach[followerGroup]{member}
      \Trigger[lease]{nodeId} \EndTrigger
    \EndForEach
    \TriggerS[leaseTimeout]{}\EndTriggerS
  \Else
    \State $isLeader := false;$
  \EndIf
\EndUponS

\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Eventual Leader Selection - Follower} 
\label{follower}
\begin{algorithmic}[1]

\UponS[init]{id}
  \State $isFolower := false;$
  \State $selfId := Id;$ $pendingPromise := Nil;$
  \State $gradientSample := \emptyset;$
\EndUponS

\Upon[gradientSample]{sample}
  \State $gradientSample := sample;$
 \EndUpon

\Upon[promiseReq]{nodeId, round}
  \If{$!isFollower$ AND $pendingPromise = Nil$ AND \emph{highestUtility}$(nodeId, gradientSample)$}
    \State $pendingPromise := nodeId;$
    \Trigger[promiseAck]{selfId, round}\EndTrigger
    \TriggerS[promiseTimeout]{}\EndTriggerS
  \Else
     \Trigger[promiseNack]{selfId, round}\EndTrigger
  \EndIf
\EndUpon

\UponS[promiseTimeout]{}
  \State $pendingPromise := Nil;$
\EndUponS

\Upon[lease]{nodeId} 
  \If{$pendingPromise = nodeId $}
    \TriggerS[followerLeaseTimeout]{}\EndTriggerS
    \TriggerS[cancelPromiseTimeout]{}\EndTriggerS
    \State $isFollower := true;$
    \State \emph{setFollowerUtility}$();$
  \EndIf
\EndUpon

\UponS[promiseTimeout]{}
  \State $pendingPromise := Nil;$
\EndUponS

\UponS[leaseTimeout]{} 
  \State $isFollower := false;$
  \State \emph{resetFollowerUtility}$();$
\EndUponS

\end{algorithmic}
\end{algorithm}


\end{document}

